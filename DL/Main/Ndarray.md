>[!Note] 深度学习存储和操作数据的主要接口是张量（$n$维数组）。它提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转换其他Python对象。
## 张量基础
### 数据类型：
在 PyTorch 中，如果列表中包含不同类型的元素，PyTorch 会自动将所有元素转换为兼容的类型。
布尔值 `True` 会被转换为整数1，`False`会被转换为整数0。
数据类型通常会被提升为 `int64`。
张量的类型通常是`int64/32,float32/64,bool`
```python
torch.tensor[1,2,3,True,4]
```

### 结构：
#### 大小与形状：
+ 大小：`X.numel()`求元素个数，返回`int`，值为`(m*n*...)`
+ 形状：`X.shape`求`size`，返回`tensor.Size([m,n,...])`
#### 维度：
维度`dim=x`，最大的中括号内是`dim=0`，最小的中括号内`dim`最大，`dim=0`对应行。
##### 相关操作：
`X.sum()`对张量的所有值代数求和（降维操作）
## 构造张量
### 函数方法：
```python
torch.zeros(6,dtype=)
torch.arange(6,dtype=torch.int32)
torch.randn(5,6,7,dtype=torch.float32)
torch.tensor([1,2,3,5])
```
### 判别式方法：
以`X == Y`为例：对于每个位置，如果`X`和`Y`在该位置相等，则新张量中相应项的值为1。这意味着逻辑语句`X == Y`在该位置处为真，否则该位置为0。
还有`>,<`等

---
## 运算：
### 算数计算：
相同`size`的`tensor`对相同位置的标量可以加减乘除，`tensor`可与常数基本运算。
单`tensor`运算：`torch.exp(a)`对`a(tensor)`求指数运算
### 联结（concatenate）：
```python
a=torch.cat((e,f,...),dim=0)
```
### 广播：

- “广播”这一术语用于描述如何在**形状不一**的数组上应用算术运算。  
- 在满足特定限制的前提下，**较小的数组“广播至”较大的数组**，使两者形状互相兼容。广播提供了一个向量化数组操作的机制，这样遍历就发生在C层面，而不是Python层面。广播可以避免不必要的数据复制，通常导向高效的算法实现。不过，也存在不适用广播的情形（可能导致拖慢计算过程的低效内存使用）。  
- 可广播的一对张量需满足以下规则：  
	- 每个张量至少有一个维度。  
	- 迭代维度尺寸时，从==**尾部**的维度开始==，维度尺寸
	    ​ 或者**相等**，  
	    ​ 或者**其中一个张量的维度尺寸为 1** ，  
	    ​ 或者**其中一个张量不存在这个维度**。  
    

```python
import torch

# 示例1：相同形状的张量总是可广播的，因为总能满足以上规则。
x = torch.empty(5, 7, 3)
y = torch.empty(5, 7, 3)


# 示例2：不可广播（ a 不满足第一条规则）。
a = torch.empty((0,))
b = torch.empty(2, 2)


# 示例3：m 和 n 可广播：
m = torch.empty(5, 3, 4, 1)
n = torch.empty(   3, 1, 1)
# 倒数第一个维度：两者的尺寸均为1
# 倒数第二个维度：n尺寸为1
# 倒数第三个维度：两者尺寸相同
# 倒数第四个维度：n该维度不存在


# 示例4：不可广播，因为倒数第三个维度：2 != 3
p = torch.empty(5, 2, 4, 1)
q = torch.empty(   3, 1, 1)
```

- 现在你对“可广播”这一概念已经有所了解了，让我们看下，**广播后的张量是什么样的**。  
- 如果张量x和张量y是可广播的，那么广播后的张量尺寸按照如下方法计算：  
- **如果x和y的维数不等，在维数较少的张量上添加尺寸为 1 的维度。结果维度尺寸是x和y相应维度尺寸的较大者。**
```python
# 示例5：可广播
c = torch.empty(5, 1, 4, 1)
d = torch.empty(   3, 1, 1)
(c + d).size()  # torch.Size([5, 3, 4, 1])


# 示例6：可广播
f = torch.empty(      1)
g = torch.empty(3, 1, 7)
(f + g).size()  # torch.Size([3, 1, 7])


# 示例7：不可广播
o = torch.empty(5, 2, 4, 1)
u = torch.empty(   3, 1, 1)
(o + u).size()

# 报错：
# ---------------------------------------------------------------------------
#
# RuntimeError                              Traceback (most recent call last)
#
# <ipython-input-17-72fb34250db7> in <module>()
#       1 o=torch.empty(5,2,4,1)
#       2 u=torch.empty(3,1,1)
# ----> 3 (o+u).size()
#
# RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1
```

---
## 索引与切片
[[切片]]
切片`a[1:2,0:,:4,:]`，切片中`:`含义近于省略与不等号，支持批量赋值。
索引类似坐标，如`a[1,2,3]` （等价于）`a[1][2][3]`，支持直接赋值。

#### 衍生节省内存方法
我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如`Y[:] = <expression>`，内存位置不变。
[[尾递归方式]]

## 一些函数办法
`X[1,1,...].item()` 是 PyTorch 张量（tensor）的方法，用于将**只包含一个元素的张量**转换为对应的 Python 标量（如 float 或 int），可以配合切片使用。与`int(),float()`作用一致。


